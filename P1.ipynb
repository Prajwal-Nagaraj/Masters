{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwal-Nagaraj/Masters/blob/ML-Project/P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Data Science Portfolio - Part I \n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd19ef4-a930-49de-bb9e-179d17849fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eee9948-2671-49ab-9b01-161d0780339a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "\n",
        "\n",
        "df = pd.read_csv('data_portfolio_21.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNfbxg2X3nzK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtCUL853Ln"
      },
      "source": [
        "### P1.1.1 - Faved by as lists \n",
        "\n",
        "The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n",
        "\n",
        "```python\n",
        "'[\"user1\", \"user2\" ... ]'\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```python\n",
        "[\"user1\", \"user2\" ... ]\n",
        "```\n",
        "\n",
        "**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJLEddGE56qw"
      },
      "outputs": [],
      "source": [
        "def transform_faves(df):\n",
        "    '''Function used to form a new column that contains the users data as a list'''\n",
        "    # your code here\n",
        "    df['subr_faved_by_as_list'] = [x.strip('[]').split(',') for x in df['subr_faved_by']]   #Using strip and the split function along with list comprehension to get the column as a list\n",
        "    return df\n",
        "\n",
        "df = transform_faves(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Merge titles and text bodies \n",
        "\n",
        "All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n",
        "\n",
        "**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n",
        "\n",
        "- 1) Wrap the title between `<title>` and `</title>` tags.\n",
        "- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n",
        "- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n",
        "- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsmY-JB39N2m"
      },
      "outputs": [],
      "source": [
        "def concat(df):\n",
        "    # your code here\n",
        "    '''Function used to wrap the title and the selftext columns if they are present'''\n",
        "    for index, row in df.iterrows():                                            #using the iterrows method to iterate through the rows of the dataframe\n",
        "      if (int(len(row['title'].strip())>1)):                                    #condition to check if a title exists\n",
        "        f_t = \"<title>\" + row['title'] + \"</title>\"                             #if a title exists, wrap it between '<title>' and '</title>'\n",
        "      if (int(len(row['selftext'].strip())>1)):                                 #Condition to check if selftext exists\n",
        "        f_t = f_t + \"\\n\"                                                        #if selftext exists, then add an escape sequence to the full text\n",
        "        f_t = f_t + \"<selftext>\" + row['selftext'] + \"</selftext>\"              #then wrap the self text between <selftext> and combine it with the full text\n",
        "      df.at[index, 'full_text'] = f_t                                           #.at method to add the full text row to the corresponding index\n",
        "    return df\n",
        "\n",
        "df = concat(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWvbAIe4TVd"
      },
      "source": [
        "### P1.1.3 - Enrich posts \n",
        "We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n",
        "\n",
        "**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nDnaSwI46T_"
      },
      "outputs": [],
      "source": [
        "def enrich_posts(df):\n",
        "    # your code here\n",
        "    '''Function used to tokenize and pos_tag the title and selftext columns'''\n",
        "    df['token_title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis = 1)        #applying tokenisation to the title row using the lambda method\n",
        "    df['token_selft'] = df.apply(lambda row: nltk.word_tokenize(row['selftext']), axis = 1)     #applying tokenisation to the selftext row using the lambda method\n",
        "    df['enriched_title'] = df.apply(lambda row: pos_tag(row['token_title']), axis = 1)          #applying pos_tagging to the tokenised title using lambda method\n",
        "    df['enriched_selftext'] = df.apply(lambda row:pos_tag(row['token_selft']), axis = 1)        #applying pos_taggin to the tokenisex self text using lambda method\n",
        "    return df\n",
        "\n",
        "df = enrich_posts(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (12 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Users with best scores (3 marks)\n",
        "\n",
        "- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhW8Rr6QSXDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6982bed-26fd-4370-cd42-8ef96b51efe9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BlanketMage': 250375,\n",
              " 'DaFunkJunkie': 218846,\n",
              " 'Dajakesta0624': 211611,\n",
              " 'JLBesq1981': 210824,\n",
              " 'NewAltWhoThis': 143538,\n",
              " 'None': 122464,\n",
              " 'NotsoPG': 118595,\n",
              " 'OldFashionedJizz': 81245,\n",
              " 'SUPERGUESSOUS': 79560,\n",
              " 'SonictheManhog': 64398,\n",
              " 'TheGamerDanYT': 58235,\n",
              " 'TheJeck': 57107,\n",
              " 'TrumpSharted': 47989,\n",
              " 'Wagamaga': 47455,\n",
              " 'apocalypticalley': 26058,\n",
              " 'chrisdh79': 25357,\n",
              " 'hildebrand_rarity': 21154,\n",
              " 'hilltopye': 18518,\n",
              " 'iSlingShlong': 18116,\n",
              " 'jigsawmap': 13677,\n",
              " 'kevinmrr': 12771,\n",
              " 'rspix000': 11900,\n",
              " 'stem12345679': 11613,\n",
              " 'tefunka': 10382}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# your code here\n",
        "'''A query used to find the highest aggregate scores in descending order'''\n",
        "a = df.groupby(by = 'author')['score'].sum().reset_index()                      #Using groupby to group by author, and the sum of their score and converting it to a dataframe\n",
        "a1 = a[a['score']>10000]                                                        #Filtering the dataframe to return only users with more than 10000 score \n",
        "a12_dict = dict(zip(a1.author, a1.score.sort_values(ascending = False)))        #Converting the dataframe to a dictionary and ordering the users by descending order of their scores\n",
        "a12_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOFrPFQT5cZ"
      },
      "source": [
        "### P1.2.2 - Awarded posts \n",
        "\n",
        "Find the number of posts that have received at least one award. Your query should return only one value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fVuaWmmUGVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf93955-813b-4cb2-e048-3d91e229f658"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# your code here\n",
        "'''A query used to return the posts that have won atleat one award'''\n",
        "df.total_awards_received[df.total_awards_received>0].count()      #Counting the number of people who have won atleast one award"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 Find Covid \n",
        "Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n",
        "\n",
        "```python\n",
        "  {'Coronavirus':'Place to discuss all things COVID-related',\n",
        "  ...\n",
        "  }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6fIWO8BUhu3"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPttp2-fsXG"
      },
      "source": [
        "### P1.2.4 - Redditors that favorite the most\n",
        "\n",
        "Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n",
        "\n",
        "```python\n",
        "     redditor\t    numb_favs\n",
        "0\tuser1           7\n",
        "1\tuser2           6\n",
        "2\tuser3\t       5\n",
        "3\tuser4           4\n",
        "...\n",
        "```\n",
        "\n",
        "where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFeie3jip44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba1361-17c4-422c-e759-b2592551f339"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 'magnusthered15'    7\n",
              " 'KarmaFury'         6\n",
              " 'Flippy-Fish'       6\n",
              " 'OmniusQubus'       6\n",
              " 'hmhmhm2'           6\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# your code here\n",
        "x = df[[ 'subreddit', 'subr_faved_by_as_list']].drop_duplicates(subset = ['subreddit'])['subr_faved_by_as_list'] # Taking only the subreddit and subr_faved_by_as_list columns separately, while dropping all the duplicate subreddit\n",
        "user_likes = pd.Series([p for q in x for p in q]).value_counts() #Using list comprehension to get the users who have favourited the most number of subreddits\n",
        "user_likes.to_frame().reset_index()                              #Making a dataframe out of it \n",
        "user_likes.columns = ['redditor', 'numb_favs']                   #And assigning column names\n",
        "user_likes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "## P1.3 Ethics \n",
        "\n",
        "**(updated on 16/03/2022)**\n",
        "\n",
        "Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n",
        "\n",
        " - Your client is a political party concerned about misinformation.\n",
        " - The project requires mining Facebook, Reddit and Instagram data.\n",
        " - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework. \n",
        "\n",
        "Your answer should address the following:\n",
        "\n",
        " - Identify the action **in which your project is the weakest**.\n",
        " - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n",
        " - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "---\n",
        "The 5 actions stated in the UK Data Ethics Framework are defining public benefit and user need, involving diverse expertise, complying with the law, checking the quality and limitations of the data, evaluating and considering wider policy implications. In my opinion the actions in which this project appears to be weakest is Reviewing the quality of data and limitations of the data and Involving Diverse Expertise.\n",
        "\n",
        "1) Reviewing the quality of data:\n",
        "\n",
        "The data sources being considered for this project are the different subreddits from reddit and although some are related to the main aim of this project(covid 19), other subreddits however are randomly selected and have very little correlation with the main aim of the project and hence violate the Article 5(1)(c) [link text](https://gdpr-info.eu/art-5-gdpr/) of the GDPR.\n",
        "\n",
        "Fairness:\n",
        "\n",
        "From the ethics framework to ensure fairness, the data being used should be assessed for bias, but from the project description there is no mention of any checks to ensure that there's no bias. So there is a possibility that data collected could be biased towards or against a certain demographic.\n",
        "\n",
        "Accountability:\n",
        "\n",
        "As the team mainly consists of people from data science background, there's no external scrutiny to make sure that the algorithm is achieving correct output, but the procedure we've followed can be easily reproduced if the dataset is in the same exact format as there are functions which can be called wherever needed.\n",
        "\n",
        "Transparency:\n",
        "\n",
        "There's no Data Sharing Agreements mentioned anywhere and I do not know if the organisations from which this data is extracted are even aware that I am using their data to scan social media for misinformation. I could publish the data and the model to public websites but as the collected data is from social media and sensitive, I feel like people could be targeted easily by just searching for them on the respective sites.\n",
        "\n",
        "My Solution:\n",
        "\n",
        "As the data is sensitive, I would first anonymise the data to make sure that the people are not targeted then I would sign a Data Sharing Agreement then assign a digital object identifier(DOI) which will allow me to share the data openly without any repercussions to the people.\n",
        "\n",
        "2) Involving Diverse Expertise:\n",
        "\n",
        "From the question we can see that the team consists of 3 people who are from different countries and cultural backgrounds but their expertise fall under datascience. And according to the UK ethics we should employ people who are beyond data scientists such as policy experts and practitioners and subject matter experts this will ensure that bias is minimised.\n",
        "\n",
        "Fairness:\n",
        "\n",
        "Although the people in my team are from different cultural backgrounds which may seem like it's diverse enough, their technical backgrounds are all under the data science umbrella and will make the team homogenous and may produce bias.\n",
        "\n",
        "Accountability:\n",
        "\n",
        "There are no external domain experts involved in the project and there's no mention of consulting a relevant civil society either. We can consider consulting the target audience or the users of this project.\n",
        "\n",
        "Transparency:\n",
        "\n",
        "We can make an effort to publish information on expert consultations but as there's no information about whether or not we're going to use expert consultations we should first hire an expert consultation and then publish information regarding the consultation.\n",
        "\n",
        "My solution:\n",
        "\n",
        "If all the 3 members in the team are of the same gender, then I would look for people of other genders to work on this project to reduce bias as much as I can and I would also consult people beyond data scientists such as ethicists, researchers and subject matter experts etc. I would also publish information on github crediting the work to everyone that have worked on this project.\n",
        "\n",
        "\n",
        "Your answer here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "P1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}